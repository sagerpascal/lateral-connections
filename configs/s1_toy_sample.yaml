dataset:
  name: straightline
  augmentation: None
  loader: torch
  batch_size: 128
  num_workers: 0

run:
  n_epochs: 15
  current_epoch: 0
  plots:
    enable: False
    only_last_epoch: False
    store_path: null
  store_state_path: None
  load_state_path: None


metrics:
  - mse:
      type: MeanSquaredError
      meter: avg
  - mae:
      type: MeanAbsoluteError
      meter: avg

optimizers:
  l2_opt:
    type: Adam
    params:
      lr: 0.001
      betas: [ 0.9, 0.999 ]
      eps: 0.00000001
      weight_decay: 0
      amsgrad: False
    scheduler:
      type: ReduceLROnPlateau
      params:
        mode: min
        factor: 0.1
        patience: 5
        threshold: 0.0001
        threshold_mode: rel
        cooldown: 0
        min_lr: 0.000001
        eps: 0.00000001

logging:
  wandb:
    active: False,
    save_dir: "../wandb"
    project: "lateral_connections_toy_example"
    log_model: True
    group: null # "hebbian_learning"
    job_type: null # "train"
  console:
    active: True

feature_extractor:
  out_channels: 4
  add_bg_channel: False

lateral_model:
  channels: 4
  max_timesteps: 5  # 1 = only one forward pass without recurrent connection
  # l1_type: 'lateral_efficient'
  # l1_params:
  #   locality_size: 2
  #   lr: 0.00005
  #   mask_out_bg: False
  #   mov_avg_rate_prev_view: 0.
  #   mov_avg_rate_prev_time: 0.5
  #   mov_avg_rate_prev: 0.
  #   hebbian_rule: 'instar'
  #   use_bias: False
  #   k: 4  # 1=WTA, 4 = No WTA (without BG Channel), 5= NO WTA (with BG Channel)
  #   thr_rate: 0.05
  #   target_rate: null # [0.05, 0.05, 0.05, 0.05, 0.8]
  #   w_init: 'identity'
  l1_type: 'lateral_flex'
  l1_params:
    locality_size: 3
    lr: 0.0005
    hebbian_rule: 'vanilla'